{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>Team name: Turquoise</h1> </center>\n",
    "\n",
    "<img src=\"images/k.jpg\" alt=\"drawing\" width=\"800\"/>\n",
    "\n",
    "\n",
    "- Members:\n",
    "\t- **Rabia Yasa Kostas**\n",
    "\t\t- e-mail: rabia.yasakostas@ed.ac.uk\n",
    "\t\t- phone: +44 7427 011498\n",
    "\t- **Kahraman Kostas**\n",
    "\t\t- e-mail: kk97@hw.ac.uk\n",
    "\t\t- phone: +44 7401 541751\n",
    "\n",
    "The team presenter: **Kahraman Kostas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> <h1>TASK 1</h1> </center> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: it is sufficient to run all cells from top to bottom, taking into account the bolded notes.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps in the first task can be summarized as follows.\n",
    "- Input files are read, using those files :\n",
    "    - Training data feature extraction is performed.\n",
    "    - Testing data feature extraction is performed.\n",
    "- At the end of feature extraction, two new dataset with 11 features is created (training data contains real distance, testing data does not).\n",
    "- An artificial neural network (ANN) is created and trained with the Training data.\n",
    "- Using the trained model, the test data is labelled and the submission file is created.\n",
    "\n",
    "These steps are illustrated in the image below.\n",
    "\n",
    "<img src=\"images/task-1.png\" alt=\"drawing\" width=\"1000\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Installing modules\n",
    "We used [Python 3.6.5](https://www.python.org/downloads/) to create the application file. We included some additional modules that were not included in the example file given at the start of the competition. These modules can be listed as:\n",
    "\n",
    "\n",
    "| Molules | Task |\n",
    "| ------ | ------ |\n",
    "|[ tensorflow ](https://www.tensorflow.org/)| Deep Learning|\n",
    "|[ Pandas  ](https://pandas.pydata.org/pandas-docs/stable/install.html)|  Data Analysis|\n",
    "|[ SciPy ](https://scipy.org/) |Distance Computing|\n",
    "\n",
    "\n",
    "\n",
    "We started with the installation of these modules as the first step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.1 Installing modules\n",
    "!pip install tensorflow==2.6.2\n",
    "!pip install scipy\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2  Setting Random Seeds\n",
    "In this step, we fixed the related random seed to be used in order to obtain repeatable results. In this way, we have provided a deterministic path where we get the same result in every run. However, according to our observations, the results obtained with different computers may differ slightly (±1%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.2  Setting Random Seeds\n",
    "seed_value=0\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "session_conf = tf.compat.v1.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)\n",
    "sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Loading the data\n",
    "\n",
    "In this section, we load the data that we will use. We took the code and explanations from the sample file given (`Task1-IPS-Challenge-2021.ipynb`).\n",
    "\n",
    "The `task1_fingerprints.json` contains all the fingerprint information for the problem. That is each entry represents a real scan of the WiFi emitters in an area of the mall. You will find that the same MAC addresses will be present in many of the fingerprints.\n",
    "\n",
    "The `task1_train.csv` contains the valid training pairs to help you design/train your algorithm. Each `id1-id2` pair has a labelled ground truth distance (in metres) and each id corresponds to a fingerprints from `task1_fingerprints.json`.\n",
    "\n",
    "The `task1_test.csv` is the same format as `task1_train.csv` but doesn't have the displacements included. \n",
    "\n",
    "\n",
    "**Please make sure there are `task1_train.csv`, `task1_test.csv`,`task1_fingerprints.json` files in the `for_contestants` folder!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unzip required files\n",
    "import zipfile\n",
    "with zipfile.ZipFile(\"for_contestants.zip\", 'r') as zip_ref:\n",
    "    zip_ref.extractall(\"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "879824it [00:43, 20404.39it/s] \n",
      "5160445it [02:21, 36491.30it/s] \n"
     ]
    }
   ],
   "source": [
    "## 1.3 Loading the data\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "path_to_data = \"for_contestants\"\n",
    "\n",
    "with open(os.path.join(path_to_data,\"task1_fingerprints.json\")) as f:\n",
    "    fps = json.load(f)\n",
    "    \n",
    "with open(os.path.join(path_to_data,\"task1_train.csv\")) as f:\n",
    "    train_data = []\n",
    "    train_h = csv.DictReader(f)\n",
    "    for pair in tqdm(train_h):\n",
    "        train_data.append([pair['id1'],pair['id2'],float(pair['displacement'])])\n",
    "        \n",
    "with open(os.path.join(path_to_data,\"task1_test.csv\")) as f:\n",
    "    test_h = csv.DictReader(f)\n",
    "    test_ids = []\n",
    "    for pair in tqdm(test_h):\n",
    "        test_ids.append([pair['id1'],pair['id2']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Feature Extraction\n",
    "\n",
    "In this step, we perform feature extraction using two functions. `feature_extraction_file` function simply pulls the relevant values of the fingerprints (in pairs) from the JSON file and sends them to the `feature_extraction`  function to do the calculations .\n",
    "\n",
    "\n",
    "In the `feature_extraction` function, if these two fingerprints are different from each other in terms of size and the devices they contain, all the devices included in the two fingerprints are brought together to form a common sequence without repeating. In each array, we make these two arrays identical (in terms of devices they include) by assigning the value 0 to the non-corresponding devices. This process is explained with an example in the following image.\n",
    "\n",
    "<img src=\"images/fp.png\" alt=\"The process of making two different fingerprints similar\" width=\"600\"/>\n",
    "\n",
    "The distance between these two fingerprints, which are made similar, is calculated using 11 different methods\\[1\\]. These methods are:\n",
    "\n",
    "- [Bray-Curtis distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.braycurtis.html)\n",
    "- [Canberra distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.canberra.html)\n",
    "- [Chebyshev distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.chebyshev.html)\n",
    "- [City Block (Manhattan) distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cityblock.html)\n",
    "- [Correlation distance ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.correlation.html)\n",
    "- [Cosine distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.cosine.html)\n",
    "- [Euclidean distance  ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.euclidean.html)\n",
    "- [Jensen-Shannon distance ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.jensenshannon.html)\n",
    "- [Minkowski distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.minkowski.html)\n",
    "- [Squared Euclidean distance](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.sqeuclidean.html)\n",
    "- [Weighted Minkowski distance ](https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.distance.wminkowski.html)\n",
    "\n",
    "\n",
    "Then, these values are directed to the `feature_extraction_file` function and saved as a CSV file within this function. In other words, fingerprints of various sizes turn into an 11-feature CSV file as a result of this process. The model to be used is trained and tested with these newly created features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.4 Feature Extraction\n",
    "import scipy.spatial\n",
    "def feature_extraction_file(data,name,flag):\n",
    "    features = [[\"braycurtis\",\n",
    "    \"canberra\",\n",
    "    \"chebyshev\",\n",
    "    \"cityblock\",\n",
    "    \"correlation\",\n",
    "    \"cosine\",\n",
    "    \"euclidean\",\n",
    "    \"jensenshannon\",\n",
    "    \"minkowski\",\n",
    "    \"sqeuclidean\",\n",
    "    \"wminkowski\", \"real\"]]\n",
    "    for i in tqdm((data), position=0, leave=True):\n",
    "        fp1 = fps[i[0]]\n",
    "        fp2 = fps[i[1]]\n",
    "        feature=feature_extraction(fp1,fp2) \n",
    "        if flag:\n",
    "            feature.append(i[2])\n",
    "        else:feature.append(0)\n",
    "        features.append(feature)\n",
    "    with open(name, \"w\", newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(features) \n",
    "    #print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.4 Feature Extraction\n",
    "def feature_extraction(fp1,fp2):\n",
    "    mac=set(list(fp1.keys())+list(fp2.keys()))\n",
    "    mac = { i : 0 for i in mac }\n",
    "    f1=mac.copy()\n",
    "    f2=mac.copy()\n",
    "    for key in fp1:\n",
    "        f1[key]=fp1[key]\n",
    "    for key in fp2:\n",
    "        f2[key]=fp2[key]        \n",
    "    \n",
    "    f1=list(f1.values())\n",
    "    f2=list(f2.values())\n",
    "\n",
    "\n",
    "    \n",
    "    braycurtis=scipy.spatial.distance.braycurtis(f1,f2)\n",
    "    canberra=scipy.spatial.distance.canberra(f1,f2)\n",
    "    chebyshev=scipy.spatial.distance.chebyshev(f1,f2)\n",
    "    cityblock=scipy.spatial.distance.cityblock(f1,f2)\n",
    "    correlation=scipy.spatial.distance.correlation(f1,f2)\n",
    "    cosine=scipy.spatial.distance.cosine(f1,f2)\n",
    "    euclidean=scipy.spatial.distance.euclidean(f1,f2)\n",
    "    jensenshannon=scipy.spatial.distance.jensenshannon(f1,f2)\n",
    "    minkowski=scipy.spatial.distance.minkowski(f1,f2)\n",
    "    sqeuclidean=scipy.spatial.distance.sqeuclidean(f1,f2)\n",
    "    wminkowski=scipy.spatial.distance.wminkowski(f1,f2,1, np.ones(len(f1)))\n",
    "\n",
    "\n",
    "\n",
    "    output_data=[braycurtis,\n",
    "        canberra,\n",
    "        chebyshev,\n",
    "        cityblock,\n",
    "        correlation,\n",
    "        cosine,\n",
    "        euclidean,\n",
    "        jensenshannon,\n",
    "        minkowski,\n",
    "        sqeuclidean,\n",
    "        wminkowski]\n",
    "    output_data = [0 if x != x else x for x in output_data]\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Model \n",
    "In this task, there is fingerprints scans which have RRSI signals from WiFi emitters surroundings in the mall. First challange wants us to estimate the distance between two fingerprints scans which is a regression task. We used ANN (Artificial Neural Networks) which is inspired by biological neural network. ANN consists of three layers; input layer, hidden layers (more than one) and output layer. ANN starts with input layer which includes the training data (with features), passes the data to the first hidden layer where the data is computed by the first hidden layer's weights. In hidden layers, there is an iteration of calculation of weights to the inputs and then apply them an activation function \\[2\\]. As our problem is regression, our last layer is a single output neuron: its output is the predicted the distances between pairs of fingerprint scans. Our first hidden layer has 64 and the second has 128 neurons. The all architecture of this model is shared as follows.\n",
    "<img src=\"images/model.png\" alt=\"The process of making two different fingerprints similar\" width=\"400\"/>\n",
    "\n",
    "\n",
    "We perform  deep learning using two functions.The `create_model` function shapes the training data to train the model and determines the structure of the model. The  `model_features` function produces a model with the specified structure. The created model is saved to be used after being trained by the `create_model` function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.5 Model \n",
    "import scipy.spatial\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "#from keras.utils.vis_utils import plot_model\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "def model_features(i,ii):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(i, input_shape=(11, ), activation='relu', name='dense_1'))\n",
    "    model.add(Dense(ii, activation='relu', name='dense_2'))\n",
    "    model.add(Dense(1, activation='linear', name='dense_output'))\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    model.summary()\n",
    "    #plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)\n",
    "    #print(model.get_config())\n",
    "    return model\n",
    "\n",
    "def create_model (name):     \n",
    "    df = pd.read_csv(name)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df=df.fillna(0)\n",
    "    X =df[df.columns[0:-1]]\n",
    "    X_train=np.array(X)\n",
    "    y_train=np.array(df[df.columns[-1]])\n",
    "\n",
    "    model=model_features(64,128)\n",
    "    history = model.fit(X_train, y_train, epochs=19, validation_split=0.5)#,batch_size=1)\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']    \n",
    "\n",
    "\n",
    "    my_xticks=list(range(len(loss)))\n",
    "    plt.figure(figsize=(20,5))\n",
    "    plt.plot(my_xticks,loss, linestyle='-', marker='o', color='b',label= \"train\")\n",
    "    plt.plot(my_xticks,val_loss, linestyle='-', marker='o', color='r',label= \"val\")\n",
    "    plt.title(\"Scores \")\n",
    "    plt.legend(numpoints=1)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.xticks(rotation=90) \n",
    "    plt.ylim([100, 150]) \n",
    "    plt.show()\n",
    "    madelname=\"./THEMODEL\"\n",
    "    model.save(madelname)\n",
    "    print(\"Model Created!\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Checking the inputs\n",
    "\n",
    "This function checks if the training and testing data have gone through feature extraction. If they have not, it creates these files and the model by calling the corresponding functions. After handling the model and all feature extraction, it formats the test data to produce the final results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.6 Checking the inputs\n",
    "from numpy import inf\n",
    "from numpy import nan\n",
    "\n",
    "def create_new_files(train,test):\n",
    "    model_path=\"./THEMODEL/\"\n",
    "    my_train_file='new_train_features.csv'\n",
    "    my_test_file='new_test_features.csv'\n",
    "    \n",
    "    \n",
    "    \n",
    "    if os.path.isfile(my_train_file) :\n",
    "        pass  \n",
    "    else:\n",
    "\n",
    "        print(\"Please wait! Training data feature extraction is in progress...\\nit will take about 10 minutes\")\n",
    "        feature_extraction_file(train,my_train_file,1)\n",
    "        print(\"TThe training feature extraction completed!!!\")       \n",
    "\n",
    "        \n",
    "\n",
    "    if os.path.isfile(my_test_file) :\n",
    "        pass \n",
    "    else:\n",
    "\n",
    "        print(\"Please wait! Testing data feature extraction is in progress...\\nit will take about 100-120 minutes\")\n",
    "        feature_extraction_file(test,my_test_file,0)\n",
    "        print(\"The testing feature extraction completed!!!\")           \n",
    "    \n",
    "\n",
    "    if  os.path.isdir(model_path):\n",
    "        pass  \n",
    "    else:\n",
    "\n",
    "        print(\"Please wait! Creating the deep learning model...\\nit will take about 10 minutes\")\n",
    "        create_model(my_train_file)\n",
    "        print(\"The model file created!!!\\n\\n\\n\")   \n",
    "    \n",
    "    model = keras.models.load_model(model_path)\n",
    "    df = pd.read_csv(my_test_file)\n",
    "    df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    df=df.fillna(0)\n",
    "    X_train =df[df.columns[0:-1]]\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(df[df.columns[-1]])\n",
    "    predicted=model.predict(X_train)  \n",
    "    print(\"Please wait! Creating resuşts... \")\n",
    "      \n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.7 Submission \n",
    "This step triggers feature extraction and model creation processes and allows all processes to begin. So, using the IDs from the `test1_test.csv` file it fills the third (displacement) column with the estimated distance for this fingerprint pairs and it saves this file in the directory with the name `TASK1-MySubmission.csv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|█                                                                     | 81482/5160445 [00:00<00:06, 808905.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait! Creating resuşts... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 5160445/5160445 [00:09<00:00, 525830.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process finished. Preparing result file ...\n",
      "The results are ready.\n",
      " See MySubmission.csv\n"
     ]
    }
   ],
   "source": [
    "## 1.7 Submission \n",
    "distance_estimate = create_new_files(train_data,test_ids)\n",
    "count=0\n",
    "\n",
    "output_data = [[\"id1\", \"id2\", \"displacement\"]]\n",
    "for id1, id2 in tqdm(test_ids):   \n",
    "    output_data.append([id1,id2,distance_estimate[count][0]])\n",
    "    count+=1\n",
    "print(\"Process finished. Preparing result file ...\")  \n",
    "with open(\"TASK1-MySubmission.csv\", \"w\", newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(output_data)\n",
    "print(\"The results are ready.\\n See MySubmission.csv\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "\\[1\\] P. Virtanen  and SciPy 1.0 Contributors. SciPy 1.0: Fundamental Algorithms for Scientific Computing in Python. Nature Methods, 17:261--272, 2020.\n",
    "\n",
    "\\[2\\] A. Geron, Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow: Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media, 2019"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
